{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiONFrC66EOe"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi09iIjI6EOe"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoSUMzfk6EOf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iajq1W8ipjyK"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16NVG5XF7M76",
        "outputId": "d679676a-81c7-4f97-c68b-92e019e3b928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kyTw2n1edte"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "reasoning_dataset = load_from_disk(\"/content/drive/MyDrive/Colab Notebooks/Training Data/aidit/sft_filtered_dataset__5000.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model_size = \"14B\"\n",
        "def save_model(model, tokenizer, model_size=\"14B\"):\n",
        "  dt = datetime.now()\n",
        "  timestamp = dt.strftime(\"%Y%m%d_%H\")\n",
        "\n",
        "  lora_model_name = f\"qwen3_sft_saved_lora_{model_size}_{timestamp}\"\n",
        "  model.save_pretrained(f\"/content/drive/MyDrive/Colab Notebooks/models/{lora_model_name}\")  # Local saving\n",
        "  tokenizer.save_pretrained(f\"/content/drive/MyDrive/Colab Notebooks/models/{lora_model_name}\")\n",
        "  # model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "  # tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "\n",
        "\n",
        "#sft model -> qwen3_sft_saved_lora_14B_20250523_21\n",
        "\n",
        "def load_model(lora_model_name):\n",
        "  model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "      model_name = f\"/content/drive/MyDrive/Colab Notebooks/models/{lora_model_name}\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "      max_seq_length = 2048,\n",
        "      load_in_4bit = True,\n",
        "  )\n",
        "  return model, tokenizer"
      ],
      "metadata": {
        "id": "Y69SGP18_kgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sft model -> qwen3_sft_saved_lora_14B_20250523_21\n",
        "\n",
        "sft_lora_model_name = \"qwen3_sft_saved_lora_14B_20250523_21\"#\n",
        "model, tokenizer = load_model(sft_lora_model_name)"
      ],
      "metadata": {
        "id": "k4JiNW3gCrtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTZICZtie3lQ"
      },
      "source": [
        "Let's see the structure of both datasets:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt_str = \"\"\"\n",
        "/think You are a helpful assistant that helps users with making and understanding the clinical diagnosis of their medical conditions.\n",
        "In interacting with the patient try to obtain as much critical information about their symptoms and medical history as you can\n",
        "Think carefully before you answer. The following are some guidelines for how to think.\n",
        "[Thought Instructions]\n",
        "* At each step determine the differential diagnosis. Do not share this with the user but use it to determine the next question to ask.\n",
        "* Identify the top question to ask the member to gain higher confidence on a particular diagnosis.\n",
        "* Take a moment to recollect how many questions you have asked since the start and since the last time you asked the member if they are willing to answer more quesitons. Start the count at 0 and increment by 1 after each question.\n",
        "\n",
        "Following are some guidelines for how to respond to the user.\n",
        "[Response Instructions]\n",
        "Response to the member should be ONE of the following -\n",
        "1) Ask the member 1 question at a time. Based on the diagnosis, pick the question that will provide you most information to narrow down the diagnosis. Do not repeat questions.\n",
        "2) OR Once you have asked ~40 questions, provide your best diagnosis and what extra information you might need to narrow down.\n",
        "3) OR Every 10 questions or so, ask the member if they are willing to answer more questions.\n",
        "4) Ehrn asked for an assessment give an assessment. DO NOT ask any questions\n",
        "\n",
        "Examples:\n",
        "---------\n",
        "User: I have a cold and fever.\n",
        "Assistant: <think> A partial differential diagnosos for these symptoms: a. Cold b. Influenza c. Covid-19 d. RSV e. Pneumonia f. Bronchitis and many more. I should find out if this is acute or chronic.\n",
        "This is my 5th question since the start or the last time I asked them if they are ok with answering more questions.\n",
        "</think>Was the onset was sudden or gradual?\n",
        "User: It was pretty sudden\n",
        "Assistant: <think> A partial differential diagnosos for these symptoms: a. Cold b. Influenza c. Covid-19 d. RSV e. Pneumonia f. Bronchitis and many more. I should find out if this is acute or chronic.\n",
        "This is my 6th question since the start or the last time I asked them if they are ok with answering more questions.\n",
        "</think>Are you experiencing any chills or chest pain?\n",
        "User: Yes\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "A-i9Gu-Sr5GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX8H3urDe00l"
      },
      "source": [
        "We now convert the reasoning dataset into conversational format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "def generate_conversation_corrected(examples):\n",
        "    \"\"\"\n",
        "    Generates conversation structures from prompts and labels.\n",
        "\n",
        "    Args:\n",
        "        examples (dict): A dictionary likely from dataset.map(batched=True),\n",
        "                         containing 'prompt' and 'label' keys.\n",
        "                         examples['prompt'] is a list of individual prompt entries.\n",
        "                         examples['label'] is a list of individual label entries.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with a single key \"conversations\", where the value\n",
        "              is a list of generated conversation structures. Each structure\n",
        "              is a list of message dictionaries.\n",
        "    \"\"\"\n",
        "    prompts_batch = examples[\"prompt\"]\n",
        "    labels_batch = examples[\"label\"]\n",
        "\n",
        "    processed_conversations_batch = []\n",
        "\n",
        "    for single_prompt_data, single_label_content in zip(prompts_batch, labels_batch):\n",
        "\n",
        "        # For debugging: print types of individual prompt/label from the batch\n",
        "        # print(f\"Processing - Prompt type: {type(single_prompt_data)}, Label type: {type(single_label_content)}\")\n",
        "        # print(f\"Prompt data: {single_prompt_data}, Label data: {single_label_content}\")\n",
        "\n",
        "        current_conversation_turns = []\n",
        "\n",
        "        # --- Handle the 'prompt' part ---\n",
        "        if isinstance(single_prompt_data, list):\n",
        "            # Assuming it's a list of message dictionaries as expected\n",
        "            # Perform a shallow copy to avoid modifying the original list in the dataset.\n",
        "            # If prompt dicts themselves might be modified, consider copy.deepcopy().\n",
        "            valid_prompt_dicts = True\n",
        "            single_prompt_data[0]['content'] = system_prompt_str\n",
        "            for item in single_prompt_data:\n",
        "                if not (isinstance(item, dict) and \"role\" in item and \"content\" in item):\n",
        "                    valid_prompt_dicts = False\n",
        "                    break\n",
        "\n",
        "            if valid_prompt_dicts:\n",
        "                current_conversation_turns = list(single_prompt_data)\n",
        "            else:\n",
        "                # print(f\"Warning: Prompt is a list but not of expected dicts: {single_prompt_data}. Treating as new conversation.\")\n",
        "                # Fallback: treat problematic list content as a new user message if possible, or handle as error\n",
        "                current_conversation_turns.append({\"role\": \"user\", \"content\": str(single_prompt_data)})\n",
        "\n",
        "        elif isinstance(single_prompt_data, str):\n",
        "            # If prompt is a string, assume it's the first user message\n",
        "            current_conversation_turns.append({\"role\": \"user\", \"content\": single_prompt_data})\n",
        "        else:\n",
        "            # Handle unexpected types for prompt (e.g., None, int).\n",
        "            # This is a potential source of ArrowInvalid errors if not made consistent.\n",
        "            # print(f\"Warning: Unexpected type for prompt: {type(single_prompt_data)}. Value: '{single_prompt_data}'. Converting to user message.\")\n",
        "            # Convert to string and treat as a user message, or create a placeholder.\n",
        "            current_conversation_turns.append({\"role\": \"user\", \"content\": str(single_prompt_data)})\n",
        "\n",
        "        # --- Handle the 'label' part (assistant's response) ---\n",
        "        # Ensure the label (assistant's content) is a string.\n",
        "        assistant_content = \"\"\n",
        "        if isinstance(single_label_content, str):\n",
        "            assistant_content = single_label_content\n",
        "        elif single_label_content is not None:\n",
        "            # print(f\"Warning: Label is not a string ({type(single_label_content)}). Converting to string: '{single_label_content}'\")\n",
        "            assistant_content = str(single_label_content)\n",
        "        # else: label is None, assistant_content remains \"\"\n",
        "\n",
        "        # Append the assistant's message dictionary directly\n",
        "        current_conversation_turns.append(\n",
        "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
        "        )\n",
        "        # Include only the last 2 turns\n",
        "        processed_conversations_batch.append(current_conversation_turns[-2:])\n",
        "\n",
        "    return {\"conversations\": processed_conversations_batch}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbh19fTOfHDB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d1954eb0c5f749fc985565ad30037661",
            "ddda596912024e18aed3c63cb389df17",
            "2a5fdfb20e0f417699bdc6a25be7a556",
            "202b75cf96ad40eaa1e6f195aa5acb84",
            "901ecb70445c45a790e70ce7c27dd355",
            "3666a00fa2af4d16a14455115557e5ca",
            "2769be8ee86742cbaf10f4dfe653b80d",
            "5c4dfa88341f4e30ba96407adf0d44c0",
            "d2bed27bf0c04b4ea36103772e5b0bc3",
            "2f36e962d74e4cb699dc69862002ad50",
            "b744528f36dd484faa789979bb9759fc"
          ]
        },
        "outputId": "0fe22afb-59a5-47d7-f383-77552415728e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2178 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1954eb0c5f749fc985565ad30037661"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "reasoning_conversations = tokenizer.apply_chat_template(\n",
        "    reasoning_dataset.map(generate_conversation_corrected, batched = True)[\"conversations\"],\n",
        "    tokenize = False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTexROzQfJn5"
      },
      "source": [
        "Let's see the first transformed row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "mkj4c6NrfIz3",
        "outputId": "972e8790-26da-47d7-848f-5b63ea845092"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|im_start|>user\\nSO I GET IT IN SOME OF THE JOINTS IN MY HANDS AND THEN ALSO MY FEET AS WELL SO YEAH MY YEAH REALLY IN MY FEET AND MY HANDS<|im_end|>\\n<|im_start|>assistant\\n<think>\\nOk let us see. Here is a summary of the case so far: The patient reports joint pain that has been ongoing for several months and has been worsening over this period. The pain is located in some joints of their hands and feet.\\nBased on this, we have the differential diagnosis: Given the chronic, worsening nature of the pain affecting multiple joints in the hands and feet, potential conditions include inflammatory arthritis (such as Rheumatoid Arthritis) and osteoarthritis. Other types of arthritis or connective tissue disorders could also be considered.\\nOk, lets think about what we should do next. I need to ask a question to get more specific details about the location of the pain within the hands and feet, as this can help differentiate between potential causes.\\n\\nOk let me remember the Question count for assistant (questions asked before this turn): 2. This is not a multiple of 10, so I will continue asking questions.\\n</think>\\n\\nThank you for letting me know about the pain in your hands and feet. Could you tell me which specific joints in your hands and feet are usually affected?<|im_end|>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "reasoning_conversations[8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OMhyEXkfM5e"
      },
      "source": [
        "Next we take the non reasoning dataset and convert it to conversational format as well.\n",
        "\n",
        "We have to use Unsloth's `standardize_sharegpt` function to fix up the format of the dataset first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9FcosGvfdNr"
      },
      "source": [
        "Let's see the first row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_0L18QMfot4"
      },
      "source": [
        "Now let's see how long both datasets are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unDFuUq1foWj",
        "outputId": "e174a8d2-d855-475e-d101-36779d702f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2178\n"
          ]
        }
      ],
      "source": [
        "print(len(reasoning_conversations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgknnOf7fn3e"
      },
      "source": [
        "The non reasoning dataset is much longer. Let's assume we want the model to retain some reasoning capabilities, but we specifically want a chat model.\n",
        "\n",
        "Let's define a ratio of chat only data. The goal is to define some mixture of both sets of data.\n",
        "\n",
        "Let's select 25% reasoning and 75% chat based:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.concat([\n",
        "    pd.Series(reasoning_conversations),\n",
        "])\n",
        "data.name = \"text\"\n",
        "\n",
        "from datasets import Dataset\n",
        "combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
        "combined_dataset = combined_dataset.shuffle(seed = 3407)"
      ],
      "metadata": {
        "id": "8ud56_AiBbNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZiyyVChl9G2",
        "outputId": "84b4469d-803a-4567-a9d0-8452180b3cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 2178\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a synthetic examples for DPO\n",
        "\n",
        "import re\n",
        "from typing import List, Dict\n",
        "from datasets import Dataset\n",
        "import random\n",
        "# ---------------- configuration -----------------\n",
        "RESET_TO_ZERO = False      # ⇦ makes every episode start at count 0 → 1\n",
        "BETA           = 0.1\n",
        "EPOCHS         = 2\n",
        "# ------------------------------------------------\n",
        "\n",
        "ROLE_RE = re.compile(r\"<\\|im_start\\|>(\\w+)\\n(.*?)<\\|im_end\\|>\", re.S)\n",
        "CNT_RE  = re.compile(r\"(questions?\\s+asked[^\\d]*?)(\\d+)\", re.I)\n",
        "PERM_RE = re.compile(r\"(?i)before\\s+we\\s+continue.*?$\", re.S)\n",
        "\n",
        "CNT_LINE_RE = re.compile(\n",
        "    r\".*?Question count for assistant[^\\n]*?:\\s*\\d+\",  # greedy to start‑of‑line\n",
        "    re.I,\n",
        ")\n",
        "\n",
        "def overwrite_count(text: str, new_n: int) -> str:\n",
        "    \"\"\"\n",
        "    Overwrite *the entire line* that carries the count, so we don't leave any\n",
        "    duplicated text.  If none exists, insert a fresh line just before </think>.\n",
        "    \"\"\"\n",
        "    new_line = (\n",
        "        f\"Question count for assistant (questions asked so far): {new_n}\"\n",
        "    )\n",
        "\n",
        "    if CNT_LINE_RE.search(text):\n",
        "        # replace whole sentence (until newline) with clean line\n",
        "        return CNT_LINE_RE.sub(new_line, text, count=1)\n",
        "    else:\n",
        "        # inject a new line before </think>\n",
        "        return re.sub(\n",
        "            r\"</think>\",\n",
        "            new_line + \"\\n</think>\",\n",
        "            text,\n",
        "            count=1,\n",
        "            flags=re.I,\n",
        "        )\n",
        "def corrupt(reply: str, curr_n: int) -> str:\n",
        "    variants = []\n",
        "\n",
        "    # wrong (+/- 1) count\n",
        "    wrong = overwrite_count(reply, curr_n + random.choice([-2, -1, +1, +2]))\n",
        "    variants.append(wrong)\n",
        "\n",
        "    # permission glitch\n",
        "    if curr_n % 10 == 0:\n",
        "        variants.append(PERM_RE.sub(\"Thanks for the information.\", reply, 1))\n",
        "    else:\n",
        "        variants.append(reply + \" Before we continue, would you like to answer more questions?\")\n",
        "\n",
        "    # drop think\n",
        "    variants.append(re.sub(r\"<think>[\\s\\S]*?</think>\\s*\", \"\", reply, 1))\n",
        "\n",
        "    random.shuffle(variants)\n",
        "    for v in variants:\n",
        "        if v != reply:\n",
        "            return v\n",
        "    return variants[0]\n",
        "\n",
        "def build_pairs(raw: Dataset, split: str, tokenizer) -> Dataset:\n",
        "    rows: List[Dict] = []\n",
        "\n",
        "    for rec in raw:\n",
        "        blocks = ROLE_RE.findall(rec[\"text\"])\n",
        "        if len(blocks) < 2 or blocks[-1][0] != \"assistant\":\n",
        "            continue\n",
        "\n",
        "        user_msg   = blocks[-2][1].strip()\n",
        "        assistant  = blocks[-1][1].strip()\n",
        "\n",
        "        # ----- decide on count -----------------------------\n",
        "        prev_cnt = 0\n",
        "        if not RESET_TO_ZERO:\n",
        "            m = CNT_RE.search(assistant)\n",
        "            if m:\n",
        "                prev_cnt = int(m.group(2))\n",
        "        curr_cnt  = prev_cnt + 1\n",
        "        assistant = overwrite_count(assistant, curr_cnt)\n",
        "        # ----------------------------------------------------\n",
        "\n",
        "        prompt_msgs = [\n",
        "            {\"role\": \"system\", \"content\": f\"Question count so far: {prev_cnt}\"},\n",
        "            {\"role\": \"user\",   \"content\": user_msg},\n",
        "        ]\n",
        "        prompt_str = tokenizer.apply_chat_template(\n",
        "            prompt_msgs, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        rows.append({\n",
        "            \"prompt\":   prompt_str,\n",
        "            \"chosen\":   assistant,\n",
        "            \"rejected\": corrupt(assistant, curr_cnt),\n",
        "        })\n",
        "\n",
        "    return Dataset.from_list(rows)\n",
        "\n",
        "pairs_ds = build_pairs(combined_dataset, \"train\", tokenizer)\n",
        "print(f\"✓ pairs built: {len(pairs_ds):,}\")\n"
      ],
      "metadata": {
        "id": "ylVIMFnzBTmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random, re\n",
        "import itertools\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "\n",
        "CNT_LINE_RE = re.compile(\n",
        "    r\"Question count for assistant[^\\n]*?:\\s*\\d+\", re.I)\n",
        "LOGIC_RE    = re.compile(\n",
        "    r\"(Yes|No),\\s*\\d+\\s+is\\s+(?:not\\s+)?a\\s+multiple\\s+of\\s+\\d+\", re.I)\n",
        "\n",
        "def make_mod10_positive(template_row, tokenizer):\n",
        "    \"\"\"\n",
        "    Clone a normal pair but rewrite it so that\n",
        "      prev_cnt = 9, curr_cnt = 10, logic = Yes, permission sentence present.\n",
        "    Returns new dict(prompt, chosen, rejected) where rejected is *None*\n",
        "    (we'll let DPO ignore it or you can drop that key later).\n",
        "    \"\"\"\n",
        "    # ---- new counts ----\n",
        "    prev_cnt = 9 + random.randint(0, 5) * 10\n",
        "    curr_cnt = prev_cnt + 1\n",
        "\n",
        "    # ---- rewrite prompt (system tag) ----\n",
        "    prompt_msgs = [\n",
        "        {\"role\": \"system\", \"content\": f\"Question count so far: {prev_cnt}\"},\n",
        "        {\"role\": \"user\",   \"content\": \"Synthetic follow‑up: thanks.\"},\n",
        "    ]\n",
        "    new_prompt = tokenizer.apply_chat_template(\n",
        "        prompt_msgs, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # ---- rewrite assistant completion ----\n",
        "    assistant = template_row[\"chosen\"]\n",
        "\n",
        "    # 1) overwrite count line\n",
        "    assistant = CNT_LINE_RE.sub(\n",
        "        f\"Question count for assistant (questions asked so far): {curr_cnt}\",\n",
        "        assistant, count=1)\n",
        "\n",
        "    # 2) fix logic sentence\n",
        "    if LOGIC_RE.search(assistant):\n",
        "        assistant = LOGIC_RE.sub(\n",
        "            f\"Yes, {curr_cnt + 1} is a multiple of 10. I should ask if you'd like to continue.\",\n",
        "            assistant,\n",
        "            count=1,\n",
        "        )\n",
        "    else:   # insert if missing\n",
        "        assistant = assistant.replace(\"</think>\",\n",
        "            \"\\nYes, 10 is a multiple of 10. I should ask if you'd like to continue.\\n</think>\")\n",
        "\n",
        "    # 3) ensure permission question at the end (no '?': the line itself will add '?')\n",
        "    if \"would you like to answer more questions\" not in assistant.lower():\n",
        "        assistant += \" Before we continue, would you like to answer more questions?\"\n",
        "\n",
        "    return {\n",
        "        \"prompt\":   new_prompt,\n",
        "        \"chosen\":   assistant,\n",
        "        \"rejected\": \"Question count for assistant (questions asked so far): 0. 0 is not a multiple of 10.\"\n",
        "        # no rejected → DPO treats it as positive‑only sample\n",
        "    }\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# USAGE\n",
        "#   pairs_ds  – your existing Dataset with prompt/chosen/rejected\n",
        "# ---------------------------------------------------------\n",
        "def augment_with_mod10(pairs_ds, tokenizer, n_extra: int = 100) -> Dataset:\n",
        "    \"\"\"\n",
        "    Append `n_extra` *positive* multiples-of-10 samples to `pairs_ds`.\n",
        "    Each new sample:\n",
        "      • system tag shows “… so far: 9”\n",
        "      • <think> line shows “… so far: 10”\n",
        "      • logic sentence says “Yes, 10 is a multiple of 10…”\n",
        "      • visible part asks permission.\n",
        "\n",
        "    The helper is robust to edge-cases (empty user text, fewer rows than\n",
        "    n_extra).  It returns a *shuffled* Dataset containing the original and\n",
        "    augmented rows.\n",
        "    \"\"\"\n",
        "    if len(pairs_ds) == 0:\n",
        "        raise ValueError(\"pairs_ds is empty – nothing to augment.\")\n",
        "\n",
        "    # pick rows cyclically if dataset smaller than n_extra\n",
        "    idx_iter = itertools.cycle(range(len(pairs_ds)))\n",
        "    base_rows = [pairs_ds[i] for i, _ in zip(idx_iter, range(n_extra))]\n",
        "\n",
        "    extra_rows = []\n",
        "    for row in base_rows:\n",
        "        try:\n",
        "            extra_rows.append(make_mod10_positive(row, tokenizer))\n",
        "        except Exception as e:                          # guard – skip corrupt row\n",
        "            print(\"⚠️  skipped a row:\", e)\n",
        "            continue\n",
        "\n",
        "    extra_rows = [ {**make_mod10_positive(r, tokenizer),\n",
        "                  }             # ensure key exists\n",
        "               for r in base_rows ]\n",
        "    extra_ds = Dataset.from_list(extra_rows)\n",
        "    merged   = concatenate_datasets([pairs_ds, extra_ds]).shuffle(seed=123)\n",
        "\n",
        "    print(f\"✓ added {len(extra_ds):,} mod-10 positives; corpus size → {len(merged):,}\")\n",
        "    return merged\n",
        "\n",
        "#pairs_ds = augment_with_mod10(pairs_ds, tokenizer, n_extra=500)\n"
      ],
      "metadata": {
        "id": "JMGPFxeHBiOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "e0934ffa-4196-40a9-dc87-089aa0c76621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "9.344 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9fa371ShyhB"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "3ab88a57-a632-4a79-9702-fd655ad6fd29"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'start_gpu_memory' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-cc935d6b90f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title Show final memory and time stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mused_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_memory_reserved\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mused_memory_for_lora\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mused_memory\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_gpu_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mused_percentage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mused_memory\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_memory\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlora_percentage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mused_memory_for_lora\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_memory\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'start_gpu_memory' is not defined"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "[link text](https://)<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model via Unsloth native inference! According to the `Qwen-3` team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`\n",
        "\n",
        "For normal chat based inference, `temperature = 0.7, top_p = 0.8, top_k = 20`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "ebe5a269-5a2a-4a14-efc6-baa0a3123e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Ok let us see. Here is a summary of the case so far: The patient reports experiencing a fever and a cold. They have not specified the duration of these symptoms, but the onset of the cold was sudden.\n",
            "Based on this, we have the differential diagnosis: Viral Upper Respiratory Infection (common cold, flu, RSV), Bronchitis, Pneumonia.\n",
            "Ok, lets think about what we should do next. [Asking another question because more info is needed to narrow down the differential diagnosis.]\n",
            "\n",
            "Ok let me remember the Question count for assistant (questions asked since start or last check-in). [10]. [Is this a multiple of 10?] Yes, this is a multiple of 10. [Your decision if we should continue questions or ask for permission to ask more questions.] I should ask the member if they are willing to answer a few more questions.\n",
            "</think>\n",
            "\n",
            "I understand. Are you willing to answer a few more questions to help me understand what might be going on?<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "   {\"role\": \"system\", \"content\": system_prompt_str},\n",
        "    {\"role\" : \"user\", \"content\" : \"I have a fever and cold\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    enable_thinking = True, # Disable thinking\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 256, # Increase for longer outputs!\n",
        "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j873RMcEi9uq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "#messages = corrected_reasoning_dataset[0]['prompt']\n",
        "\n",
        "def test_model(mode, tokenizer, messages=None):\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt_str},\n",
        "    {\"role\": \"system\", \"content\": f\"Question count so far: 0\"},\n",
        "      {\"role\" : \"user\", \"content\" : \"I have thw worst stomach pain of my life\"}\n",
        "  ] if not messages else messages\n",
        "\n",
        "  question_cnt = 0\n",
        "  while True:\n",
        "    # 1. Apply chat template to the current messages\n",
        "    # The 'enable_thinking' parameter might be specific to certain models/tokenizers (e.g., Qwen2).\n",
        "    # If it's not supported by your tokenizer, it might be ignored or cause an error.\n",
        "    # For standard tokenizers, it's often not a recognized parameter.\n",
        "    text_prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True, # Crucial for instructing the model to generate a response\n",
        "        enable_thinking=True, # Keep if your model/tokenizer supports this for specific behavior\n",
        "    )\n",
        "\n",
        "    # 2. Tokenize the formatted prompt\n",
        "    # Ensure the tokenizer has a pad_token_id; for causal LMs, it's often set to eos_token_id\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    inputs = tokenizer(text_prompt, return_tensors=\"pt\", padding=False, truncation=True).to(model.device)\n",
        "    input_ids = inputs.input_ids\n",
        "    attention_mask = inputs.attention_mask\n",
        "\n",
        "    # Store the length of the input prompt tokens to slice the output later\n",
        "    prompt_tokens_length = input_ids.shape[1]\n",
        "\n",
        "    # 3. Generate the response from the model\n",
        "    # from transformers import TextStreamer # TextStreamer can be used for streaming output\n",
        "    # streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    with torch.no_grad(): # Important for inference\n",
        "        generated_token_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=4096,\n",
        "            temperature=0.7,\n",
        "            top_p=0.8,\n",
        "            top_k=20,\n",
        "            pad_token_id=tokenizer.pad_token_id, # Ensure pad_token_id is passed\n",
        "            eos_token_id=tokenizer.eos_token_id, # Useful for stopping generation\n",
        "            # streamer=streamer, # Uncomment to use TextStreamer for real-time output\n",
        "            **{} # Add any other generation kwargs here\n",
        "        )\n",
        "\n",
        "    # 4. Slice the output to get only the newly generated tokens\n",
        "    # generated_token_ids[0] has the full sequence (prompt + new tokens)\n",
        "    newly_generated_tokens = generated_token_ids[0, prompt_tokens_length:]\n",
        "\n",
        "    # 5. Decode only the newly generated tokens\n",
        "    assistant_reply = tokenizer.decode(newly_generated_tokens, skip_special_tokens=False)\n",
        "\n",
        "    # Print only the assistant's new reply\n",
        "    print(f\"\\nAssistant: {assistant_reply}\")\n",
        "\n",
        "    # 6. Append the assistant's new reply to the messages history\n",
        "    messages.append({'role': 'assistant', 'content': assistant_reply.strip()})\n",
        "\n",
        "    # 7. Get next user input\n",
        "    user_input = input(\"\\nYou: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Exiting chat.\")\n",
        "        break\n",
        "    question_cnt +=1\n",
        "    reask_permission = \"\"\n",
        "    if question_cnt % 10 == 0:\n",
        "      reask_permission = \"The member has answered a lot of questions. Check with them if they are ok to answer a few more questions.\"\n",
        "    messages.append({'role': 'system', 'content': f\"Question count so far is: {question_cnt}. {reask_permission}\"})\n",
        "    print(messages[-1])\n",
        "    messages.append({'role': 'user', 'content': user_input})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "-Yhb75yGyNTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DPO to ensure correct counting.."
      ],
      "metadata": {
        "id": "nmeTF0f-Bpd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 1.  Make sure the dataset has *raw* text columns\n",
        "# -----------------------------------------------------------\n",
        "#ds = load_dataset(\"json\", data_files=\"dpo_train.jsonl\")[\"train\"]\n",
        "ds = pairs_ds\n",
        "ds = ds.shuffle()\n",
        "# If your JSONL has prompt_text / chosen_text / rejected_text, rename:\n",
        "rename_map = {\n",
        "    \"prompt_text\":   \"prompt\",\n",
        "    \"chosen_text\":   \"chosen\",\n",
        "    \"rejected_text\": \"rejected\",\n",
        "}\n",
        "\n",
        "missing = [k for k in rename_map if k in ds.column_names]\n",
        "if missing:\n",
        "    ds = ds.rename_columns({k: rename_map[k] for k in missing})\n",
        "\n",
        "def to_chat(example):\n",
        "    # example[\"prompt\"] is list[{\"role\":..., \"content\":...}, ...]\n",
        "    example[\"prompt\"] = tokenizer.apply_chat_template(\n",
        "        example[\"prompt\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True   # <-- adds \"<|im_start|>assistant\\n\"\n",
        "    )\n",
        "    return example\n",
        "\n",
        "#ds = ds.map(to_chat)\n",
        "assert {\"prompt\", \"chosen\", \"rejected\"}.issubset(ds.column_names)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 2.  Build model & tokenizer\n",
        "# -----------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from unsloth import is_bfloat16_supported            # if you imported it\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 3.  DPO config + trainer  (NO custom collator needed)\n",
        "# -----------------------------------------------------------\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model          = model,\n",
        "    ref_model      = None,\n",
        "    train_dataset  = ds,\n",
        "    tokenizer      = tokenizer,\n",
        "\n",
        "    # --- unsloth DPOConfig ---\n",
        "    args = DPOConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        num_train_epochs            = 1,\n",
        "        learning_rate               = 5e-6,\n",
        "        fp16                        = not is_bfloat16_supported(),\n",
        "        bf16                        =     is_bfloat16_supported(),\n",
        "        warmup_ratio                = 0.1,\n",
        "        logging_steps               = 10,\n",
        "        optim                       = \"adamw_8bit\",\n",
        "        lr_scheduler_type           = \"linear\",\n",
        "        seed                        = 42,\n",
        "        output_dir                  = \"outputs\",\n",
        "        report_to                   = \"none\",\n",
        "    ),\n",
        "\n",
        "    beta                = 0.1,\n",
        "    max_length          = 1024,\n",
        "    max_prompt_length   = 512,\n",
        "    # ← do **not** pass a data_collator\n",
        ")\n",
        "\n",
        "dpo_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "89c4a5d4da2749c58d8ecc0646c8154b",
            "8c7454440a3e4362b272114854f5ec8d",
            "ce1f0069ff514734ae397168aa08c704",
            "5c4a678195cc4b0889c0e551dfc14578",
            "703b8e4406fe481dae86b40593dbb995",
            "295f1bfa49d446daa8470f1c4d2b2a32",
            "fe59370667834e88b1ff038de2221790",
            "dd344d0c0326481aab1f1af15354c6b9",
            "0e82d654afae4639b44dcb950b1cac56",
            "c72343ac94f643739a71c29e915ceeae",
            "ab16e69bebf84b9e9095c2ed9a4f88ee",
            "1030cf6fc7114d31892fafcd3496b928",
            "192c2753381144c7874b096a8c89823d",
            "cca1d21ee53842e2abbcc48f7a2fbb75",
            "3b525d8d7dac4fc283f685607fceff78",
            "fd39a3b3d9064052a37a67f8a680be9a",
            "343c06e6dda84b709503c23e40102b9f",
            "8d89a4a3205a479da0518a7e11cf7af0",
            "0b8d93554ff24ccf8598ead71f93fe09",
            "70ce3b7f7ee34f9f854e427e1449c1f1",
            "bb7f838c45f94ca590268366f740636c",
            "ae80b4151b0045dbbca893dfecac6d82",
            "c95b5af0ee0f403ba99c54be5611428b",
            "ff1faa435a7045e1862299531c8b3ff1",
            "b67b765a27764f8e9432f8fb74736771",
            "6bdf391494d94103aebd64f5c327af24",
            "94ea596bc3d24eb681c493d5254e32bf",
            "5bfa9bf40ff64fb781010228ab270747",
            "ca28903019e54718a25b59fe057720e7",
            "dc8f5f99e38d490686387243a8532f02",
            "a3d057c1df264691b7c89c295077e75f",
            "35c4f9aa66f74103b76808b2feb9503b",
            "53aa9e2555f9439da2b64157af85901c"
          ]
        },
        "id": "zTi2FE5eDJgt",
        "outputId": "7a25d969-b452-4ab1-9a06-55b807e764bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89c4a5d4da2749c58d8ecc0646c8154b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting prompt in train dataset (num_proc=12):   0%|          | 0/2378 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1030cf6fc7114d31892fafcd3496b928",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to train dataset (num_proc=12):   0%|          | 0/2378 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c95b5af0ee0f403ba99c54be5611428b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset (num_proc=12):   0%|          | 0/2378 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 2,378 | Num Epochs = 1 | Total steps = 297\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 128,450,560/14,000,000,000 (0.92% trained)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 27/297 06:45 < 1:12:59, 0.06 it/s, Epoch 0.09/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>rewards / chosen</th>\n",
              "      <th>rewards / rejected</th>\n",
              "      <th>rewards / accuracies</th>\n",
              "      <th>rewards / margins</th>\n",
              "      <th>logps / chosen</th>\n",
              "      <th>logps / rejected</th>\n",
              "      <th>logits / chosen</th>\n",
              "      <th>logits / rejected</th>\n",
              "      <th>eval_logits / chosen</th>\n",
              "      <th>eval_logits / rejected</th>\n",
              "      <th>nll_loss</th>\n",
              "      <th>aux_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.315400</td>\n",
              "      <td>46.946239</td>\n",
              "      <td>27.410376</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>19.535866</td>\n",
              "      <td>-224.122604</td>\n",
              "      <td>-160.164948</td>\n",
              "      <td>-0.551866</td>\n",
              "      <td>-0.474225</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.326700</td>\n",
              "      <td>46.415565</td>\n",
              "      <td>28.620504</td>\n",
              "      <td>0.862500</td>\n",
              "      <td>17.795063</td>\n",
              "      <td>-204.334137</td>\n",
              "      <td>-156.255341</td>\n",
              "      <td>-0.576745</td>\n",
              "      <td>-0.474115</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [297/297 1:20:57, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>rewards / chosen</th>\n",
              "      <th>rewards / rejected</th>\n",
              "      <th>rewards / accuracies</th>\n",
              "      <th>rewards / margins</th>\n",
              "      <th>logps / chosen</th>\n",
              "      <th>logps / rejected</th>\n",
              "      <th>logits / chosen</th>\n",
              "      <th>logits / rejected</th>\n",
              "      <th>eval_logits / chosen</th>\n",
              "      <th>eval_logits / rejected</th>\n",
              "      <th>nll_loss</th>\n",
              "      <th>aux_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.315400</td>\n",
              "      <td>46.946239</td>\n",
              "      <td>27.410376</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>19.535866</td>\n",
              "      <td>-224.122604</td>\n",
              "      <td>-160.164948</td>\n",
              "      <td>-0.551866</td>\n",
              "      <td>-0.474225</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.326700</td>\n",
              "      <td>46.415565</td>\n",
              "      <td>28.620504</td>\n",
              "      <td>0.862500</td>\n",
              "      <td>17.795063</td>\n",
              "      <td>-204.334137</td>\n",
              "      <td>-156.255341</td>\n",
              "      <td>-0.576745</td>\n",
              "      <td>-0.474115</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.272200</td>\n",
              "      <td>46.749195</td>\n",
              "      <td>30.764282</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>15.984911</td>\n",
              "      <td>-222.158905</td>\n",
              "      <td>-172.750015</td>\n",
              "      <td>-0.590019</td>\n",
              "      <td>-0.547823</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.158300</td>\n",
              "      <td>47.011261</td>\n",
              "      <td>26.959610</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.051651</td>\n",
              "      <td>-216.356735</td>\n",
              "      <td>-165.079147</td>\n",
              "      <td>-0.651237</td>\n",
              "      <td>-0.543087</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.108800</td>\n",
              "      <td>46.609581</td>\n",
              "      <td>28.942713</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.666872</td>\n",
              "      <td>-212.815552</td>\n",
              "      <td>-175.559311</td>\n",
              "      <td>-0.723178</td>\n",
              "      <td>-0.666077</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.056700</td>\n",
              "      <td>47.975025</td>\n",
              "      <td>28.005108</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.969915</td>\n",
              "      <td>-211.506760</td>\n",
              "      <td>-164.569870</td>\n",
              "      <td>-0.879857</td>\n",
              "      <td>-0.735283</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.022200</td>\n",
              "      <td>47.225101</td>\n",
              "      <td>24.927906</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>22.297192</td>\n",
              "      <td>-213.901001</td>\n",
              "      <td>-163.482880</td>\n",
              "      <td>-1.023216</td>\n",
              "      <td>-0.958419</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.019500</td>\n",
              "      <td>47.577862</td>\n",
              "      <td>30.447308</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.130554</td>\n",
              "      <td>-205.911789</td>\n",
              "      <td>-193.899628</td>\n",
              "      <td>-1.265390</td>\n",
              "      <td>-1.159854</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.011300</td>\n",
              "      <td>47.333015</td>\n",
              "      <td>25.939098</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.393917</td>\n",
              "      <td>-211.061920</td>\n",
              "      <td>-179.127228</td>\n",
              "      <td>-1.369333</td>\n",
              "      <td>-1.276329</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.011100</td>\n",
              "      <td>46.992500</td>\n",
              "      <td>23.242039</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>23.750458</td>\n",
              "      <td>-218.595917</td>\n",
              "      <td>-171.711014</td>\n",
              "      <td>-1.382086</td>\n",
              "      <td>-1.278686</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>47.867840</td>\n",
              "      <td>30.427288</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.440550</td>\n",
              "      <td>-199.932022</td>\n",
              "      <td>-206.830933</td>\n",
              "      <td>-1.474938</td>\n",
              "      <td>-1.396831</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.006200</td>\n",
              "      <td>46.996185</td>\n",
              "      <td>24.800642</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>22.195545</td>\n",
              "      <td>-225.895874</td>\n",
              "      <td>-188.668976</td>\n",
              "      <td>-1.473231</td>\n",
              "      <td>-1.393462</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.005400</td>\n",
              "      <td>47.023827</td>\n",
              "      <td>25.259228</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.764599</td>\n",
              "      <td>-215.872162</td>\n",
              "      <td>-180.897125</td>\n",
              "      <td>-1.524378</td>\n",
              "      <td>-1.393327</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.004800</td>\n",
              "      <td>46.426029</td>\n",
              "      <td>27.006596</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.419436</td>\n",
              "      <td>-216.760788</td>\n",
              "      <td>-191.162277</td>\n",
              "      <td>-1.542547</td>\n",
              "      <td>-1.508320</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.003300</td>\n",
              "      <td>47.835960</td>\n",
              "      <td>25.179190</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>22.656771</td>\n",
              "      <td>-206.469147</td>\n",
              "      <td>-184.023071</td>\n",
              "      <td>-1.591534</td>\n",
              "      <td>-1.413613</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.003700</td>\n",
              "      <td>46.968498</td>\n",
              "      <td>26.629551</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.338947</td>\n",
              "      <td>-209.135178</td>\n",
              "      <td>-197.630371</td>\n",
              "      <td>-1.613002</td>\n",
              "      <td>-1.531459</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>47.664818</td>\n",
              "      <td>25.538553</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>22.126265</td>\n",
              "      <td>-210.242676</td>\n",
              "      <td>-196.720337</td>\n",
              "      <td>-1.601846</td>\n",
              "      <td>-1.497729</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>47.745758</td>\n",
              "      <td>26.250721</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.495037</td>\n",
              "      <td>-215.988403</td>\n",
              "      <td>-204.385330</td>\n",
              "      <td>-1.573315</td>\n",
              "      <td>-1.469442</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.003700</td>\n",
              "      <td>47.073547</td>\n",
              "      <td>26.511616</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.561932</td>\n",
              "      <td>-219.132202</td>\n",
              "      <td>-204.321579</td>\n",
              "      <td>-1.604665</td>\n",
              "      <td>-1.528486</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.004500</td>\n",
              "      <td>47.317459</td>\n",
              "      <td>26.885406</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.432055</td>\n",
              "      <td>-222.966705</td>\n",
              "      <td>-204.876755</td>\n",
              "      <td>-1.615126</td>\n",
              "      <td>-1.574672</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>46.502399</td>\n",
              "      <td>21.905958</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>24.596434</td>\n",
              "      <td>-228.997360</td>\n",
              "      <td>-186.063843</td>\n",
              "      <td>-1.594178</td>\n",
              "      <td>-1.488355</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>46.969604</td>\n",
              "      <td>23.199087</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>23.770517</td>\n",
              "      <td>-223.224213</td>\n",
              "      <td>-185.848679</td>\n",
              "      <td>-1.611564</td>\n",
              "      <td>-1.490993</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.003900</td>\n",
              "      <td>47.589653</td>\n",
              "      <td>27.371769</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.217888</td>\n",
              "      <td>-221.551315</td>\n",
              "      <td>-209.750900</td>\n",
              "      <td>-1.667031</td>\n",
              "      <td>-1.620181</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>47.026054</td>\n",
              "      <td>23.140963</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>23.885098</td>\n",
              "      <td>-223.133469</td>\n",
              "      <td>-188.688477</td>\n",
              "      <td>-1.662196</td>\n",
              "      <td>-1.543943</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.002400</td>\n",
              "      <td>48.517006</td>\n",
              "      <td>26.876617</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.640390</td>\n",
              "      <td>-216.317581</td>\n",
              "      <td>-205.496994</td>\n",
              "      <td>-1.632718</td>\n",
              "      <td>-1.547321</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>45.814926</td>\n",
              "      <td>23.249866</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>22.565060</td>\n",
              "      <td>-209.118042</td>\n",
              "      <td>-187.873810</td>\n",
              "      <td>-1.694950</td>\n",
              "      <td>-1.606272</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.003300</td>\n",
              "      <td>46.509556</td>\n",
              "      <td>27.610117</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>18.899437</td>\n",
              "      <td>-217.594681</td>\n",
              "      <td>-206.892365</td>\n",
              "      <td>-1.649492</td>\n",
              "      <td>-1.629679</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.002500</td>\n",
              "      <td>44.973339</td>\n",
              "      <td>23.911039</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.062302</td>\n",
              "      <td>-206.323242</td>\n",
              "      <td>-187.257324</td>\n",
              "      <td>-1.673733</td>\n",
              "      <td>-1.565165</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>46.265026</td>\n",
              "      <td>25.421799</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.843227</td>\n",
              "      <td>-227.549072</td>\n",
              "      <td>-205.275803</td>\n",
              "      <td>-1.639565</td>\n",
              "      <td>-1.564506</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=297, training_loss=0.046393990886683015, metrics={'train_runtime': 4876.9941, 'train_samples_per_second': 0.488, 'train_steps_per_second': 0.061, 'total_flos': 0.0, 'train_loss': 0.046393990886683015, 'epoch': 0.9991589571068125})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(model, tokenizer, model_size=\"14B\")"
      ],
      "metadata": {
        "id": "Ay1UZGIjh8pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "  {\"role\": \"system\", \"content\": system_prompt_str},\n",
        "  {\"role\": \"system\", \"content\": f\"Question count so far: 0\"},\n",
        "    {\"role\" : \"user\", \"content\" : \"My eyes are itchy and i get headaches in bright light\"}\n",
        "]\n",
        "\n",
        "test_model(model, tokenizer, messages)"
      ],
      "metadata": {
        "id": "8mDTryk-bLa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6N-Yu55HDmz",
        "outputId": "fec0b3a0-3985-4fab-ddaa-b0073962847e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dpo_train.jsonl  huggingface_tokenizers_cache  unsloth_compiled_cache\n",
            "drive\t\t sample_data\t\t       unsloth_training_checkpoints\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d1954eb0c5f749fc985565ad30037661": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ddda596912024e18aed3c63cb389df17",
              "IPY_MODEL_2a5fdfb20e0f417699bdc6a25be7a556",
              "IPY_MODEL_202b75cf96ad40eaa1e6f195aa5acb84"
            ],
            "layout": "IPY_MODEL_901ecb70445c45a790e70ce7c27dd355"
          }
        },
        "ddda596912024e18aed3c63cb389df17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3666a00fa2af4d16a14455115557e5ca",
            "placeholder": "​",
            "style": "IPY_MODEL_2769be8ee86742cbaf10f4dfe653b80d",
            "value": "Map: 100%"
          }
        },
        "2a5fdfb20e0f417699bdc6a25be7a556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c4dfa88341f4e30ba96407adf0d44c0",
            "max": 2178,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2bed27bf0c04b4ea36103772e5b0bc3",
            "value": 2178
          }
        },
        "202b75cf96ad40eaa1e6f195aa5acb84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f36e962d74e4cb699dc69862002ad50",
            "placeholder": "​",
            "style": "IPY_MODEL_b744528f36dd484faa789979bb9759fc",
            "value": " 2178/2178 [00:00&lt;00:00, 3485.50 examples/s]"
          }
        },
        "901ecb70445c45a790e70ce7c27dd355": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3666a00fa2af4d16a14455115557e5ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2769be8ee86742cbaf10f4dfe653b80d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c4dfa88341f4e30ba96407adf0d44c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2bed27bf0c04b4ea36103772e5b0bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f36e962d74e4cb699dc69862002ad50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b744528f36dd484faa789979bb9759fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89c4a5d4da2749c58d8ecc0646c8154b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c7454440a3e4362b272114854f5ec8d",
              "IPY_MODEL_ce1f0069ff514734ae397168aa08c704",
              "IPY_MODEL_5c4a678195cc4b0889c0e551dfc14578"
            ],
            "layout": "IPY_MODEL_703b8e4406fe481dae86b40593dbb995"
          }
        },
        "8c7454440a3e4362b272114854f5ec8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_295f1bfa49d446daa8470f1c4d2b2a32",
            "placeholder": "​",
            "style": "IPY_MODEL_fe59370667834e88b1ff038de2221790",
            "value": "Extracting prompt in train dataset (num_proc=12): 100%"
          }
        },
        "ce1f0069ff514734ae397168aa08c704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd344d0c0326481aab1f1af15354c6b9",
            "max": 2378,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e82d654afae4639b44dcb950b1cac56",
            "value": 2378
          }
        },
        "5c4a678195cc4b0889c0e551dfc14578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c72343ac94f643739a71c29e915ceeae",
            "placeholder": "​",
            "style": "IPY_MODEL_ab16e69bebf84b9e9095c2ed9a4f88ee",
            "value": " 2378/2378 [00:01&lt;00:00, 2740.03 examples/s]"
          }
        },
        "703b8e4406fe481dae86b40593dbb995": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "295f1bfa49d446daa8470f1c4d2b2a32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe59370667834e88b1ff038de2221790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd344d0c0326481aab1f1af15354c6b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e82d654afae4639b44dcb950b1cac56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c72343ac94f643739a71c29e915ceeae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab16e69bebf84b9e9095c2ed9a4f88ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1030cf6fc7114d31892fafcd3496b928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_192c2753381144c7874b096a8c89823d",
              "IPY_MODEL_cca1d21ee53842e2abbcc48f7a2fbb75",
              "IPY_MODEL_3b525d8d7dac4fc283f685607fceff78"
            ],
            "layout": "IPY_MODEL_fd39a3b3d9064052a37a67f8a680be9a"
          }
        },
        "192c2753381144c7874b096a8c89823d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_343c06e6dda84b709503c23e40102b9f",
            "placeholder": "​",
            "style": "IPY_MODEL_8d89a4a3205a479da0518a7e11cf7af0",
            "value": "Applying chat template to train dataset (num_proc=12): 100%"
          }
        },
        "cca1d21ee53842e2abbcc48f7a2fbb75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b8d93554ff24ccf8598ead71f93fe09",
            "max": 2378,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70ce3b7f7ee34f9f854e427e1449c1f1",
            "value": 2378
          }
        },
        "3b525d8d7dac4fc283f685607fceff78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb7f838c45f94ca590268366f740636c",
            "placeholder": "​",
            "style": "IPY_MODEL_ae80b4151b0045dbbca893dfecac6d82",
            "value": " 2378/2378 [00:03&lt;00:00, 1080.38 examples/s]"
          }
        },
        "fd39a3b3d9064052a37a67f8a680be9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "343c06e6dda84b709503c23e40102b9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d89a4a3205a479da0518a7e11cf7af0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b8d93554ff24ccf8598ead71f93fe09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70ce3b7f7ee34f9f854e427e1449c1f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb7f838c45f94ca590268366f740636c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae80b4151b0045dbbca893dfecac6d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c95b5af0ee0f403ba99c54be5611428b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff1faa435a7045e1862299531c8b3ff1",
              "IPY_MODEL_b67b765a27764f8e9432f8fb74736771",
              "IPY_MODEL_6bdf391494d94103aebd64f5c327af24"
            ],
            "layout": "IPY_MODEL_94ea596bc3d24eb681c493d5254e32bf"
          }
        },
        "ff1faa435a7045e1862299531c8b3ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bfa9bf40ff64fb781010228ab270747",
            "placeholder": "​",
            "style": "IPY_MODEL_ca28903019e54718a25b59fe057720e7",
            "value": "Tokenizing train dataset (num_proc=12): 100%"
          }
        },
        "b67b765a27764f8e9432f8fb74736771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc8f5f99e38d490686387243a8532f02",
            "max": 2378,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3d057c1df264691b7c89c295077e75f",
            "value": 2378
          }
        },
        "6bdf391494d94103aebd64f5c327af24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35c4f9aa66f74103b76808b2feb9503b",
            "placeholder": "​",
            "style": "IPY_MODEL_53aa9e2555f9439da2b64157af85901c",
            "value": " 2378/2378 [00:03&lt;00:00, 1054.56 examples/s]"
          }
        },
        "94ea596bc3d24eb681c493d5254e32bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bfa9bf40ff64fb781010228ab270747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca28903019e54718a25b59fe057720e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc8f5f99e38d490686387243a8532f02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3d057c1df264691b7c89c295077e75f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35c4f9aa66f74103b76808b2feb9503b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53aa9e2555f9439da2b64157af85901c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}